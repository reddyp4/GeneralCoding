{"cells":[{"cell_type":"markdown","source":["## Mounting the asset directory containing dataset"],"metadata":{"id":"-OZexFSyj4K_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZo_arxuE1o_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697267121841,"user_tz":-330,"elapsed":18961,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"bca400aa-682c-4cff-9625-34e02983d8a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import os\n","\n","local_assets_b = False\n","\n","if local_assets_b:\n","  assets_dir = \"/content/assets/P3/\"\n","\n","  if not os.path.isdir(assets_dir):\n","    assert os.path.isfile(\"assets.zip\")\n","    os.system(\"unzip assets.zip\")\n","else:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  assets_dir = '/content/drive/MyDrive/CV-1/assets/P3/'"]},{"cell_type":"markdown","metadata":{"id":"BsMdG4QTi3vd"},"source":["# Transfer Learning Introduction"]},{"cell_type":"markdown","metadata":{"id":"b5lhD-V5jDdX"},"source":["Transfer learning is a technique in deep learning where pre-trained models trained on large-scale datasets are leveraged to solve new tasks with limited labeled data. It involves taking a pre-trained model, which has learned rich and generalized features from a source task, and fine-tuning it on a target task.\n","\n","### What is the dataset being used?\n","The dataset being used is CIFAR-10 is a widely used benchmark dataset in computer vision and machine learning. It consists of 60,000 small-sized color images (32x32 pixels) belonging to 10 different classes, with 6,000 images per class. The dataset is split into a training set of 50,000 images and a test set of 10,000 images. The classes in CIFAR-10 include common objects like airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks. CIFAR-10 serves as a good dataset for evaluating and benchmarking image classification models.\n","\n","When it comes to transfer learning, VGG is often used as a backbone model. Its pre-trained weights, which have been learned on the large-scale ImageNet dataset, capture generic features like edges, textures, and shapes that are beneficial for various visual recognition tasks. By leveraging the pre-trained VGG model, we can fine-tune it on the CIFAR-10 dataset to perform image classification. The lower-level layers of VGG capture low-level features, such as edges and corners, while the higher-level layers learn more complex features. This enables VGG to extract meaningful representations from images and generalize well to new tasks with limited labeled data.\n","\n","By fine-tuning VGG on the CIFAR-10 dataset, we can take advantage of the pre-trained weights and learn task-specific features for image classification. This approach is effective when the target task has a similar domain or visual characteristics as the source task on which VGG was pre-trained. Transfer learning with VGG can help achieve better performance on CIFAR-10 by leveraging the knowledge learned from ImageNet, even with a smaller dataset."]},{"cell_type":"markdown","metadata":{"id":"oQxrW_4VRSOW"},"source":["## Transfer Learning (VGG, ResNet) vs Building a model from Scratch\n","\n","Below we will develop 3 models to show the benefits of using Transfer Learning. Transfer Learning will help us save time (which is very valuable) and cost (computation required is less, equally valuable)."]},{"cell_type":"markdown","metadata":{"id":"fS16lIq9PiFu"},"source":["### Using VGG for Transfer Learning\n","\n","\n","### What is VGG?\n","VGG (Visual Geometry Group) is a popular deep convolutional neural network (CNN) architecture developed by the Visual Geometry Group at the University of Oxford. VGGNet is known for its simplicity and effectiveness in image classification tasks. It consists of multiple convolutional layers followed by fully connected layers. The most common variant, VGG-16, has 16 layers, including 13 convolutional layers and 3 fully connected layers. VGGNet has achieved impressive results on various image classification benchmarks, including the ImageNet challenge.\n","\n","\n","### What are Residual Networks (ResNet)?\n","\n","ResNet, short for Residual Network, is a specific type of neural network that was introduced in 2015 by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their paper “Deep Residual Learning for Image Recognition”. The ResNet models were extremely successful, winning 1st place in the ILSVRC 2015 classification competition with a top-5 error rate of 3.57%. ResNet is a fully convolutional neural network with an encoder-decoder structure that has been adapted to incorporate other convolutional neural network architecture designs. The architecture of ResNet can be broadly thought of as a deep neural network with skip connections between layers that add the outputs from previous layers to the outputs of stacked layers. This results in the ability to train much deeper neural networks without running into the vanishing gradient problem. ResNet has many variants that run on the same concept but have different numbers of layers. ResNet50 is used to denote the variant that can work with 50 neural network layers. ResNet has significantly enhanced the performance of neural networks with more layers and has been used in various domains, including computer vision and biomedical imaging."]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"ssAq-Q_ZdheP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIFRLx9fTZ6G"},"outputs":[],"source":["# define the imports\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.models import vgg16\n","import numpy as np"]},{"cell_type":"code","source":["# Define a transform to preprocess the data\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform=transform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697267139386,"user_tz":-330,"elapsed":6568,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"0981b588-816d-4401-a0c4-f6343dfdb46c","id":"Veu0DrBn9zRs"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:01<00:00, 105398039.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n"]}]},{"cell_type":"markdown","source":["This below snippet helps us to calculate the mean and standard deviation of the dataset."],"metadata":{"id":"fhpVqxkY9zRs"}},{"cell_type":"code","source":["# Calculate the mean and standard deviation\n","mean = np.mean(trainset.data, axis=(0, 1, 2)) / 255.0\n","std = np.std(trainset.data, axis=(0, 1, 2)) / 255.0\n","\n","print('Mean:', mean)\n","print('Standard Deviation:', std)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697267139866,"user_tz":-330,"elapsed":481,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"55a2987f-42dc-445c-b7f0-5b8751554a3b","id":"BLrpzGzd9zRs"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean: [0.49139968 0.48215841 0.44653091]\n","Standard Deviation: [0.24703223 0.24348513 0.26158784]\n"]}]},{"cell_type":"markdown","source":["We use those values to normalize the dataset."],"metadata":{"id":"l6EpxvCi9zRt"}},{"cell_type":"markdown","source":["### Image Transformation\n","We will be performing the following transformations:\n","\n","*   Random Crop\n","*   Flip\n","*   Normalize data"],"metadata":{"execution":{"iopub.status.busy":"2023-07-08T09:14:48.812197Z","iopub.execute_input":"2023-07-08T09:14:48.812613Z","iopub.status.idle":"2023-07-08T09:14:48.851901Z","shell.execute_reply.started":"2023-07-08T09:14:48.812585Z","shell.execute_reply":"2023-07-08T09:14:48.850350Z"},"id":"Wrl9JLI79zRs"}},{"cell_type":"code","source":["transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding = 4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2434, 0.2616))\n","])"],"metadata":{"execution":{"iopub.status.busy":"2023-07-08T11:50:45.216216Z","iopub.execute_input":"2023-07-08T11:50:45.218113Z","iopub.status.idle":"2023-07-08T11:50:45.228795Z","shell.execute_reply.started":"2023-07-08T11:50:45.218086Z","shell.execute_reply":"2023-07-08T11:50:45.227633Z"},"trusted":true,"id":"fY4pdbxT9zRt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test time Transformations\n","* Normalization"],"metadata":{"id":"dH6hYSgy9zRt"}},{"cell_type":"code","source":["transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2434, 0.2616))\n","])"],"metadata":{"execution":{"iopub.status.busy":"2023-07-08T11:50:45.230531Z","iopub.execute_input":"2023-07-08T11:50:45.230971Z","iopub.status.idle":"2023-07-08T11:50:45.243388Z","shell.execute_reply.started":"2023-07-08T11:50:45.230897Z","shell.execute_reply":"2023-07-08T11:50:45.242175Z"},"trusted":true,"id":"cznmobsj9zRt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset\n","\n","Datasets are the collections of your training, validation, and test data. They consist of input samples and their corresponding target labels (for supervised learning). In PyTorch, datasets are typically created using custom classes inheriting from `torch.utils.data.Dataset`. You load your data into this class, allowing easy access during training."],"metadata":{"id":"g6eo6IYX9zRt"}},{"cell_type":"code","source":["## Preparing dataset\n","trainset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size = 128, shuffle = True, num_workers = 2)"],"metadata":{"execution":{"iopub.status.busy":"2023-07-08T11:50:45.246345Z","iopub.execute_input":"2023-07-08T11:50:45.247470Z","iopub.status.idle":"2023-07-08T11:50:55.327573Z","shell.execute_reply.started":"2023-07-08T11:50:45.247437Z","shell.execute_reply":"2023-07-08T11:50:55.326528Z"},"trusted":true,"outputId":"c13642f8-6c93-4c0d-e0e5-9544d459566d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697267140819,"user_tz":-330,"elapsed":954,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"id":"c88_0HDG9zRt"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck')"],"metadata":{"id":"BsRiq8QLQ6QP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataloaders\n","\n","Data Loaders wrap your dataset and provide functionalities for iterating through batches of data during training. They handle shuffling, batching, and parallel data loading, optimizing the data pipeline."],"metadata":{"id":"ybP2HJdbMvQ_"}},{"cell_type":"code","source":["testset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size = 256, shuffle = True, num_workers = 2)"],"metadata":{"execution":{"iopub.status.busy":"2023-07-08T11:50:55.329269Z","iopub.execute_input":"2023-07-08T11:50:55.329626Z","iopub.status.idle":"2023-07-08T11:50:56.016017Z","shell.execute_reply.started":"2023-07-08T11:50:55.329595Z","shell.execute_reply":"2023-07-08T11:50:56.014826Z"},"trusted":true,"outputId":"cdac399b-c3fc-401c-ca85-bf7658cce0cd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697267142252,"user_tz":-330,"elapsed":784,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"id":"fYMAmYWz9zRt"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["### Learning Rate\n","\n","The learning rate is a hyperparameter that controls how much the model's parameters should be updated during training."],"metadata":{"id":"xr2Q7gTPMyO8"}},{"cell_type":"code","source":["lr = 0.001"],"metadata":{"id":"Z9gSqvmp9xog"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"oBP3MLUy9xlu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loss Function\n","\n","Loss functions measure the difference between the predicted output and the actual target values. Common loss functions include Cross-Entropy Loss for classification tasks and Mean Squared Error for regression tasks."],"metadata":{"id":"1wGa5Lg3M1yf"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"do91u6HE9xbC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load the Pretrained Model"],"metadata":{"id":"bmFLI2rrQRhc"}},{"cell_type":"markdown","metadata":{"id":"5DFukhGvQOcC"},"source":["Since the last layer of VGG is 1000 (as it was trained for ImageNet which contains 1000 classes) we are removing that and connecting the second last layer to the number of classes we currently have i.e. 2."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4fbbc1d4-e1a7-462d-f0ec-ffadb6a0732d","executionInfo":{"status":"ok","timestamp":1697265073931,"user_tz":-330,"elapsed":8527,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"id":"Jr5IzphzQOcD"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:02<00:00, 276MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["device:  cuda\n"]},{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":13}],"source":["# Load the pre-trained VGG-16 model\n","vgg = vgg16(pretrained=True)\n","\n","# Modify the last layer of VGG by changing it to 10 classes instead of 1000 as trained for ImageNet\n","vgg.classifier[6] = nn.Linear(in_features=4096, out_features=len(classes))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");\n","print(\"device: \", device)\n","vgg.to(device)"]},{"cell_type":"markdown","source":["### Optimizer\n","\n","Optimizers are algorithms that adjust the model's parameters during training to minimize the loss function. Common optimizers include SGD (Stochastic Gradient Descent), Adam, and RMSprop."],"metadata":{"id":"OfWGTqanM4e7"}},{"cell_type":"code","source":["vgg_optimizer = optim.SGD(vgg.parameters(), lr = 1e-3, momentum=0.9, weight_decay = 5e-4)"],"metadata":{"id":"nMUuiWVoAY1B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Scheduler\n","\n","A scheduler adjusts the learning rate dynamically during training, allowing fine-tuning.\n","\n","Cosine Annealing: The learning rate starts high and is annealed down to a minimum value following a cosine curve. It helps the model explore the search space broadly at the beginning of training and then refine the search space as it converges.\n","\n","T_max: This parameter defines the total number of iterations it takes to complete one cycle of the cosine function. The learning rate will follow a cosine curve for the first T_max iterations and then restart the cycle.\n","\n","Here's a conceptual explanation:\n","\n","At the start of training, the learning rate is relatively high, allowing the model to explore a larger area of the loss landscape.\n","As training progresses (over the T_max iterations), the learning rate decreases following a cosine curve.\n","When T_max iterations are completed, the learning rate is at its minimum.\n","The scheduler then restarts the cosine curve, and the learning rate starts to increase again, allowing the model to explore broadly for the next cycle.\n","This approach often helps models converge more efficiently by first exploring broadly and then refining their parameters as training progresses."],"metadata":{"id":"FellmboeM7GC"}},{"cell_type":"code","source":["vgg_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(vgg_optimizer, T_max = 200)"],"metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-08T11:51:31.294517Z","iopub.execute_input":"2023-07-08T11:51:31.294858Z","iopub.status.idle":"2023-07-08T11:51:31.299127Z","shell.execute_reply.started":"2023-07-08T11:51:31.294829Z","shell.execute_reply":"2023-07-08T11:51:31.298076Z"},"trusted":true,"id":"yrkAEDhBCgYf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set the models in training mode"],"metadata":{"id":"fkcCHOKwM-Wr"}},{"cell_type":"code","source":["vgg.train()"],"metadata":{"execution":{"iopub.status.busy":"2023-07-08T11:51:33.161298Z","iopub.execute_input":"2023-07-08T11:51:33.161654Z","iopub.status.idle":"2023-07-08T11:51:33.169864Z","shell.execute_reply.started":"2023-07-08T11:51:33.161627Z","shell.execute_reply":"2023-07-08T11:51:33.168817Z"},"trusted":true,"id":"OIzSob8JCgYf","outputId":"d089dd21-51ee-4926-dd67-b71f788b6541","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697265079086,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["vgg_model = vgg.to(device)"],"metadata":{"execution":{"iopub.status.busy":"2023-07-08T11:51:33.504767Z","iopub.execute_input":"2023-07-08T11:51:33.505481Z","iopub.status.idle":"2023-07-08T11:51:36.431473Z","shell.execute_reply.started":"2023-07-08T11:51:33.505428Z","shell.execute_reply":"2023-07-08T11:51:36.429849Z"},"trusted":true,"id":"KokhMVHHCgYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_batch(epoch, model, optimizer):\n","    print(\"epoch \", epoch)\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for batch_idx, (input, targets) in enumerate(trainloader):\n","        inputs, targets = input.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","    print(batch_idx, len(trainloader), 'Train Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                         % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"],"metadata":{"id":"D6xXjwspd2pW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate_batch(epoch, model):\n","    global best_acc\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","    print(batch_idx, len(testloader), 'Validation Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))"],"metadata":{"id":"Pury4KNvd2m8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g3GRPlHLoQ4i"},"source":["### Training the Model\n","\n","Note: The below code takes around 10 mins to run, so you can load the model in case you would like to play around without waiting."]},{"cell_type":"code","source":["start_epoch = 0\n","for epoch in range(start_epoch, start_epoch+10):\n","    train_batch(epoch, vgg_model, vgg_optimizer)\n","    validate_batch(epoch, vgg_model)\n","    vgg_scheduler.step()"],"metadata":{"id":"3BNcNzOeDQiB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697265940439,"user_tz":-330,"elapsed":274149,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"50557a15-d1e8-4534-b91b-1a81836e732b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["390 391 Loss: 0.384 | Acc: 86.760% (43380/50000)\n","epoch  0\n","39 40 Loss: 0.406 | Acc: 85.780% (8578/10000)\n","390 391 Loss: 0.357 | Acc: 87.650% (43825/50000)\n","epoch  1\n","39 40 Loss: 0.391 | Acc: 86.660% (8666/10000)\n","390 391 Loss: 0.338 | Acc: 88.344% (44172/50000)\n","epoch  2\n","39 40 Loss: 0.373 | Acc: 87.460% (8746/10000)\n","390 391 Loss: 0.317 | Acc: 88.962% (44481/50000)\n","epoch  3\n","39 40 Loss: 0.373 | Acc: 87.590% (8759/10000)\n","390 391 Loss: 0.296 | Acc: 89.810% (44905/50000)\n","epoch  4\n","39 40 Loss: 0.372 | Acc: 87.470% (8747/10000)\n","390 391 Loss: 0.281 | Acc: 90.182% (45091/50000)\n","epoch  5\n","39 40 Loss: 0.364 | Acc: 88.190% (8819/10000)\n","390 391 Loss: 0.267 | Acc: 90.596% (45298/50000)\n","epoch  6\n","39 40 Loss: 0.347 | Acc: 88.080% (8808/10000)\n","390 391 Loss: 0.253 | Acc: 91.128% (45564/50000)\n","epoch  7\n","39 40 Loss: 0.370 | Acc: 88.250% (8825/10000)\n","390 391 Loss: 0.239 | Acc: 91.750% (45875/50000)\n","epoch  8\n","39 40 Loss: 0.341 | Acc: 88.690% (8869/10000)\n","390 391 Loss: 0.229 | Acc: 92.084% (46042/50000)\n","epoch  9\n","39 40 Loss: 0.339 | Acc: 88.680% (8868/10000)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6j5HYJPZGzBW"},"source":["### Save your Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBpF4S3Qvvv9"},"outputs":[],"source":["# Save the model\n","state_dict = vgg_model.state_dict()\n","torch.save(state_dict, assets_dir + \"vgg_model_state_dict.pt\")"]},{"cell_type":"markdown","metadata":{"id":"wtq3aj26G1dF"},"source":["### Load your already saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FB7NWVg8vzGy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697266034244,"user_tz":-330,"elapsed":3303,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"4da6edc6-85b4-4b37-d1b8-ae01231f97f5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":34}],"source":["from torchvision.models import vgg16\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Load the model in case training it takes a lot of time\n","file_path = assets_dir + 'vgg_model_state_dict.pt'\n","\n","loaded_vgg_model = vgg16(pretrained=False)\n","loaded_vgg_model.classifier[6] = nn.Linear(in_features=4096, out_features=len(classes))\n","\n","# Load the saved state dictionary\n","saved_state_dict = torch.load(file_path)\n","\n","# Load the state dictionary into the model\n","loaded_vgg_model.load_state_dict(saved_state_dict)\n","\n","# Set the model to evaluation mode\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","loaded_vgg_model = loaded_vgg_model.to(device)\n","loaded_vgg_model.eval()"]},{"cell_type":"markdown","metadata":{"id":"o3evm6mJOpUK"},"source":["### Developing a model from Scratch"]},{"cell_type":"markdown","metadata":{"id":"P5oLhETqp2tw"},"source":["### CNN Model Architecture"]},{"cell_type":"markdown","metadata":{"id":"EhMIWTpmOrHq"},"source":["The below CNN class is an architecture we are building from scratch in order to compare which one performs better in the task of Image Classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2aP68UfobBgE"},"outputs":[],"source":["# Define model architecture\n","class CNN(nn.Module):\n","    \"\"\"\n","    Convolutional Neural Network (CNN) for image classification.\n","\n","    This CNN consists of two convolutional layers followed by max-pooling layers,\n","    and two fully connected layers. The input images are expected to have three channels (RGB).\n","\n","    Args:\n","        num_classes (int): The number of classes in the classification task.\n","\n","    Attributes:\n","        conv1 (nn.Conv2d): The first convolutional layer with 16 output channels and a kernel size of 3x3.\n","        relu1 (nn.ReLU): The ReLU activation function applied after the first convolutional layer.\n","        pool1 (nn.MaxPool2d): The max-pooling layer with a kernel size of 2x2 after the first convolutional layer.\n","        conv2 (nn.Conv2d): The second convolutional layer with 32 output channels and a kernel size of 3x3.\n","        relu2 (nn.ReLU): The ReLU activation function applied after the second convolutional layer.\n","        pool2 (nn.MaxPool2d): The max-pooling layer with a kernel size of 2x2 after the second convolutional layer.\n","        fc1 (nn.Linear): The first fully connected layer with 64 units.\n","        relu3 (nn.ReLU): The ReLU activation function applied after the first fully connected layer.\n","        fc2 (nn.Linear): The second fully connected layer with `num_classes` units for classification.\n","\n","    Methods:\n","        forward(x): Performs a forward pass through the network given an input tensor x.\n","                    Returns the output tensor after passing through the fully connected layers.\n","    \"\"\"\n","\n","    def __init__(self, num_classes):\n","        super(CNN, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n","        self.relu2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(in_features=32 * 8 * 8, out_features=64)\n","        self.relu3 = nn.ReLU()\n","        self.fc2 = nn.Linear(in_features=64, out_features=num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool1(self.relu1(self.conv1(x)))\n","        x = self.pool2(self.relu2(self.conv2(x)))\n","        x = x.view(-1, 32 * 8 * 8)\n","        x = self.relu3(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"MPpj5l1mrkEs"},"source":["### Loss Criterion and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFzTUiFTczca"},"outputs":[],"source":["# Create the model\n","cnn_model = CNN(num_classes=len(classes)).to(device)"]},{"cell_type":"code","source":["# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"tlEQv_RTYzmD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cnn_optimizer = optim.SGD(params=cnn_model.parameters(), lr=0.001, momentum=0.9)"],"metadata":{"id":"-W79AvMRZcUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cnn_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(cnn_optimizer, T_max = 200)"],"metadata":{"id":"hmwefVbSZdBF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CzH1N8v5rdy4"},"source":["### Training the model and Evaluation"]},{"cell_type":"code","source":["start_epoch = 0\n","for epoch in range(start_epoch, start_epoch+10):\n","    train_batch(epoch, cnn_model, cnn_optimizer)\n","    validate_batch(epoch, cnn_model)\n","    cnn_scheduler.step()"],"metadata":{"id":"R2EgA_6lJYu6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697267497748,"user_tz":-330,"elapsed":254918,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"a03ca7d3-7e52-4c45-de71-0aae1133af8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["390 391 Loss: 2.205 | Acc: 20.302% (10151/50000)\n","epoch  0\n","39 40 Loss: 2.011 | Acc: 29.210% (2921/10000)\n","390 391 Loss: 1.923 | Acc: 31.176% (15588/50000)\n","epoch  1\n","39 40 Loss: 1.803 | Acc: 36.150% (3615/10000)\n","390 391 Loss: 1.754 | Acc: 36.854% (18427/50000)\n","epoch  2\n","39 40 Loss: 1.639 | Acc: 41.220% (4122/10000)\n","390 391 Loss: 1.636 | Acc: 40.864% (20432/50000)\n","epoch  3\n","39 40 Loss: 1.520 | Acc: 45.050% (4505/10000)\n","390 391 Loss: 1.565 | Acc: 43.062% (21531/50000)\n","epoch  4\n","39 40 Loss: 1.454 | Acc: 47.570% (4757/10000)\n","390 391 Loss: 1.515 | Acc: 45.288% (22644/50000)\n","epoch  5\n","39 40 Loss: 1.417 | Acc: 48.640% (4864/10000)\n","390 391 Loss: 1.476 | Acc: 46.732% (23366/50000)\n","epoch  6\n","39 40 Loss: 1.395 | Acc: 48.780% (4878/10000)\n","390 391 Loss: 1.443 | Acc: 48.088% (24044/50000)\n","epoch  7\n","39 40 Loss: 1.351 | Acc: 50.810% (5081/10000)\n","390 391 Loss: 1.410 | Acc: 49.098% (24549/50000)\n","epoch  8\n","39 40 Loss: 1.316 | Acc: 53.440% (5344/10000)\n","390 391 Loss: 1.383 | Acc: 50.386% (25193/50000)\n","epoch  9\n","39 40 Loss: 1.277 | Acc: 54.270% (5427/10000)\n"]}]},{"cell_type":"code","source":["# Save the CNN model\n","state_dict = cnn_model.state_dict()\n","torch.save(state_dict, assets_dir + \"cnn_model_state_dict.pt\")"],"metadata":{"id":"ziGNtd80A3KG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the CNN model\n","file_path = assets_dir + 'cnn_model_state_dict.pt'\n","\n","# Load a pre-trained ResNet-50 model\n","loaded_cnn_model = CNN(num_classes=len(classes))\n","\n","\n","# Load the saved state dictionary\n","saved_state_dict = torch.load(file_path)\n","\n","# Load the state dictionary into the model\n","loaded_cnn_model.load_state_dict(saved_state_dict)\n","\n","# Set the model to evaluation mode\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","loaded_resnet_model = loaded_cnn_model.to(device)\n","loaded_resnet_model.eval()"],"metadata":{"id":"l3JPvzmDEUrG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697267627735,"user_tz":-330,"elapsed":538,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"444a592e-ad6d-4bc4-ecea-fe7448b41182"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNN(\n","  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (relu1): ReLU()\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (relu2): ReLU()\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=2048, out_features=64, bias=True)\n","  (relu3): ReLU()\n","  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["### Load the pretrained ResNet Model"],"metadata":{"id":"nfID2mMWW33d"}},{"cell_type":"code","source":["from torchvision.models import resnet50\n","\n","# Load the pre-trained ResNet-50 model\n","resnet_model = resnet50(pretrained=True)\n","\n","# Modify the last layer of ResNet by adding one more fully connected layer with 512 units\n","num_features = resnet_model.fc.in_features\n","\n","resnet_model.fc = nn.Linear(num_features, len(classes))  # Output layer for your specific number of classes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hDWnOWsDZ1_8","executionInfo":{"status":"ok","timestamp":1697265364450,"user_tz":-330,"elapsed":1228,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"670baf5d-b80d-4b1d-fc1f-936ee4baca04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 236MB/s]\n"]}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"fF8Xztdldi7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resnet_optimizer = optim.SGD(resnet_model.parameters(), lr = 1e-3, momentum=0.9, weight_decay = 5e-4)"],"metadata":{"id":"L30zC3M5dtRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resnet_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(resnet_optimizer, T_max = 200)"],"metadata":{"id":"c-ZGKpP6dvOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resnet_model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1NCXVHidxZG","executionInfo":{"status":"ok","timestamp":1697265373351,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"d5f2c683-9b49-4eb2-f30a-c69e62c3b182"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["resnet_model = resnet_model.to(device)"],"metadata":{"id":"4MUIa_Y_d1q6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_epoch = 0\n","for epoch in range(start_epoch, start_epoch+10):\n","    train_batch(epoch, resnet_model, resnet_optimizer)\n","    validate_batch(epoch, resnet_model)\n","    resnet_scheduler.step()"],"metadata":{"id":"i5WhsEsqD6mB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697266374429,"user_tz":-330,"elapsed":327621,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"527031f2-1dbc-4d78-c367-428ffe048822"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["390 391 Loss: 0.415 | Acc: 85.280% (42640/50000)\n","epoch  0\n","39 40 Loss: 0.478 | Acc: 84.060% (8406/10000)\n","390 391 Loss: 0.376 | Acc: 86.782% (43391/50000)\n","epoch  1\n","39 40 Loss: 0.462 | Acc: 84.320% (8432/10000)\n","390 391 Loss: 0.350 | Acc: 87.826% (43913/50000)\n","epoch  2\n","39 40 Loss: 0.454 | Acc: 84.860% (8486/10000)\n","390 391 Loss: 0.329 | Acc: 88.260% (44130/50000)\n","epoch  3\n","39 40 Loss: 0.439 | Acc: 85.190% (8519/10000)\n","390 391 Loss: 0.308 | Acc: 89.144% (44572/50000)\n","epoch  4\n","39 40 Loss: 0.445 | Acc: 85.410% (8541/10000)\n","390 391 Loss: 0.288 | Acc: 89.902% (44951/50000)\n","epoch  5\n","39 40 Loss: 0.449 | Acc: 85.270% (8527/10000)\n","390 391 Loss: 0.268 | Acc: 90.576% (45288/50000)\n","epoch  6\n","39 40 Loss: 0.437 | Acc: 85.850% (8585/10000)\n","390 391 Loss: 0.255 | Acc: 91.098% (45549/50000)\n","epoch  7\n","39 40 Loss: 0.459 | Acc: 85.930% (8593/10000)\n","390 391 Loss: 0.235 | Acc: 91.594% (45797/50000)\n","epoch  8\n","39 40 Loss: 0.464 | Acc: 85.900% (8590/10000)\n","390 391 Loss: 0.221 | Acc: 92.088% (46044/50000)\n","epoch  9\n","39 40 Loss: 0.450 | Acc: 86.320% (8632/10000)\n"]}]},{"cell_type":"markdown","source":["### Save the Model"],"metadata":{"id":"CS6s6drIWtV_"}},{"cell_type":"code","source":["# Save the model\n","state_dict = resnet_model.state_dict()\n","torch.save(state_dict, assets_dir + \"resnet_model_state_dict.pt\")"],"metadata":{"id":"0WhPbIgMw0HS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.models import resnet50\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim"],"metadata":{"id":"ZuoJD3y_UlJg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load the ResNet model"],"metadata":{"id":"HjGuVXdAWUE7"}},{"cell_type":"code","source":["# Load the model\n","# Load the model in case training it takes a lot of time\n","file_path = assets_dir + '/resnet_model_state_dict.pt'\n","\n","# Load a pre-trained ResNet-50 model\n","loaded_resnet_model = resnet50(pretrained=False)\n","\n","# Modify the classifier layer for your specific task\n","loaded_resnet_model.fc = nn.Linear(in_features=2048, out_features=len(classes))  # Assuming you want to classify into 2 classes\n","\n","# Load the saved state dictionary\n","saved_state_dict = torch.load(file_path)\n","\n","# Load the state dictionary into the model\n","loaded_resnet_model.load_state_dict(saved_state_dict)\n","\n","# Set the model to evaluation mode\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","loaded_resnet_model = loaded_resnet_model.to(device)\n","loaded_resnet_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IRO47MFJUVbA","executionInfo":{"status":"ok","timestamp":1697266528515,"user_tz":-330,"elapsed":2600,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"fa906aa2-09b8-4d31-a602-b987d650f0de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","def evaluate(model, data_loader):\n","    model.eval()\n","    predictions = []\n","    true_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(images)\n","            # Get the predicted class with the highest probability for each image\n","            _, predicted_labels = torch.max(outputs, 1)\n","\n","            # Append the predictions\n","            predictions.extend(predicted_labels.cpu().numpy())\n","            # Append the true labels\n","            true_labels.extend(labels.cpu().numpy())\n","\n","    # Use the predictions and true labels to evaluate the model on the following\n","    accuracy = accuracy_score(y_true=true_labels, y_pred=predictions)\n","    precision = precision_score(y_true=true_labels, y_pred=predictions, average='weighted')\n","    recall = recall_score(y_true=true_labels, y_pred=predictions, average='weighted')\n","    f1 = f1_score(y_true=true_labels, y_pred=predictions, average='weighted')\n","    cf_matrix = confusion_matrix(y_true=true_labels, y_pred=predictions)\n","\n","    return accuracy, precision, recall, f1, cf_matrix"],"metadata":{"id":"PWfjyzjfaC4t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### CNN Model Evaluation"],"metadata":{"id":"zgyvb-taEfq7"}},{"cell_type":"code","source":["test_accuracy, test_precision, test_recall, test_f1, cf_matrix = evaluate(loaded_cnn_model, testloader)\n","\n","# Print final test set performance\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Test Precision: {test_precision:.4f}\")\n","print(f\"Test Recall: {test_recall:.4f}\")\n","print(f\"Test F1 Score: {test_f1:.4f}\")\n","print(f\"Test Confusion Matrix:\")\n","print(cf_matrix)"],"metadata":{"id":"xgDKQV1HEfVj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697267646066,"user_tz":-330,"elapsed":3022,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"d1954eee-2658-431b-bbf6-a0d81dd5a57a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.5427\n","Test Precision: 0.5476\n","Test Recall: 0.5427\n","Test F1 Score: 0.5305\n","Test Confusion Matrix:\n","[[584  62  71  20   5   5  33  33 135  52]\n"," [ 29 771   4  12   2   5  25  20  32 100]\n"," [ 67  27 432  76  43  90 130  94  22  19]\n"," [ 17  26  71 351  31 188 156 123  12  25]\n"," [ 33  16 205  62 211  72 171 213  11   6]\n"," [ 10  13  79 176  20 448  65 166  13  10]\n"," [  3  17  64  56  18  31 742  48   4  17]\n"," [ 14  13  32  53  30  68  36 716   3  35]\n"," [114 115  14  18   5  11  12  18 633  60]\n"," [ 39 251   8  18   4   6  34  59  42 539]]\n"]}]},{"cell_type":"markdown","source":["### VGG Model Evaluation"],"metadata":{"id":"yJd5uxobWhVT"}},{"cell_type":"code","source":["test_accuracy, test_precision, test_recall, test_f1, cf_matrix = evaluate(loaded_vgg_model, testloader)\n","\n","# Print final test set performance\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Test Precision: {test_precision:.4f}\")\n","print(f\"Test Recall: {test_recall:.4f}\")\n","print(f\"Test F1 Score: {test_f1:.4f}\")\n","print(f\"Test Confusion Matrix:\")\n","print(cf_matrix)"],"metadata":{"id":"yYpUZ_AaaFH7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697266772595,"user_tz":-330,"elapsed":3864,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"a3741798-2cc6-42e5-d026-976ed4284110"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.8868\n","Test Precision: 0.8892\n","Test Recall: 0.8868\n","Test F1 Score: 0.8869\n","Test Confusion Matrix:\n","[[926   7   9   5  15   1   3   4  29   1]\n"," [ 14 934   0   0   0   0   0   2  11  39]\n"," [ 26   1 786  35  72  23  41  11   4   1]\n"," [  8   2  15 817  32  83  19  15   4   5]\n"," [  3   1  11  17 908  13  17  27   1   2]\n"," [  1   0   8 139  31 798   5  17   0   1]\n"," [  5   1   6  32  22   6 923   2   2   1]\n"," [  9   2   5  25  25  27   2 904   1   0]\n"," [ 32   5   2   5   3   0   1   0 948   4]\n"," [ 17  29   0   3   2   0   0   2  23 924]]\n"]}]},{"cell_type":"markdown","source":["### ResNet Model Evaluation"],"metadata":{"id":"pLFoy7ITWmSQ"}},{"cell_type":"code","source":["# test_accuracy, test_precision, test_recall, test_f1, cf_matrix = evaluate(resnet, test_loader)\n","test_accuracy, test_precision, test_recall, test_f1, cf_matrix = evaluate(loaded_resnet_model, testloader)\n","# Print final test set performance\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Test Precision: {test_precision:.4f}\")\n","print(f\"Test Recall: {test_recall:.4f}\")\n","print(f\"Test F1 Score: {test_f1:.4f}\")\n","print(f\"Test Confusion Matrix:\")\n","print(cf_matrix)"],"metadata":{"id":"NRNV0JAzfv7P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697266786646,"user_tz":-330,"elapsed":3908,"user":{"displayName":"Rahul Nori","userId":"16349435476798232628"}},"outputId":"22191286-8cfb-4d09-998a-ec6a33b188e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.8632\n","Test Precision: 0.8644\n","Test Recall: 0.8632\n","Test F1 Score: 0.8631\n","Test Confusion Matrix:\n","[[906  11  24   7  11   1   4   4  22  10]\n"," [ 12 915   2   3   1   4   3   1  16  43]\n"," [ 31   0 828  29  54  15  32   9   0   2]\n"," [ 12   4  45 745  46  82  37  18   5   6]\n"," [  6   1  32  18 889   9  22  20   3   0]\n"," [  7   0  23 153  36 746  13  18   2   2]\n"," [  5   0  15  30  12   7 924   5   0   2]\n"," [  8   2  13  21  39  27   4 880   0   6]\n"," [ 65   7   4   7   5   0   2   3 895  12]\n"," [ 23  52   2   5   2   2   1   2   7 904]]\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}